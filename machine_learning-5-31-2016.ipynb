{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the Training and Evaluation Data\n",
    "\n",
    "We will use the msha.csv data set to build and evaluate our autocoder. We begin by dividing it into 3 pieces, each of which will be used for a separate purpose. These pieces are training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes:\n",
      "    test: 8643\n",
      "    validation: 9032\n",
      "    training: 18681\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read the csv file into a data frame, a special Python object for representing tabular data\n",
    "df = pd.read_csv(r'msha.csv', parse_dates=['ACCIDENT_DT'])\n",
    "\n",
    "# add a column to our data frame with the year the incident occurred\n",
    "df['YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
    "\n",
    "# split the data frame into separate test, validation, and training dataframes based on the year of the incident\n",
    "df_test = df[df['YEAR'] == 2013]\n",
    "df_validation = df[df['YEAR'] == 2012]\n",
    "df_training = df[df['YEAR'] <= 2011]\n",
    "\n",
    "print('''Data set sizes:\n",
    "    test: %s\n",
    "    validation: %s\n",
    "    training: %s''' % (len(df_test), len(df_validation), len(df_training)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "A critical step (perhaps the most important) in building our machine learning autocoder is determining the relevant inputs for our task and encoding these in a way that will be both computationally efficient, and easy for our algorithm to learn. \n",
    "\n",
    "For text classification tasks such as ours, a very simple approach that often works very well is known as the bag-of-words-representation.\n",
    "\n",
    "The basic idea is to create a 1 to 1 mapping of words to positions in a vector (list of numbers) and then to represent each collection of words with a vector where the values of the vector indicate which words occurred and did not occur.\n",
    "\n",
    "For example, for the sake of simplicity suppose there are only 5 possible words that might occur in narrative: \"he\", \"fell\", \"hit\", \"rock\", \"car\". We might map these to a vector of length 5 such that the first position of that vector always corresponds to the word \"he\", the second to \"fell\", the third to \"hit\", the fourth to \"rock, and the fifth to \"car\". \n",
    "\n",
    "<img src=\"feature_map.png\">\n",
    "\n",
    "\n",
    "\n",
    "Suppose also that we use a value of 0 to indicate when a word does not occur in a narrative, and a value of 1 to indicate when it does. If we were to receive \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "larmapping of words to positions in a vector (list of numbers) and then represent map words that might occur in a narrative to fixed positions in a vecotr (list of numbers), and then to indicate where those create a vector (i.e. list of numbers) where each position in that vector corresponds to a word that might occur, and the value at that position indicates convert each example (i.e. narrative) into a giant vector (list of numbers), where each position in the vector corresponds to one of the  words that occurs in our training data, and the value at that position indicates something about that word's occurence in the specific example. Often, we use a value of 1 to indicate that the word is present in the narrative, and a 0 to indicate it is not. The resulting vector is known as a feature vector because it represents the features of our data that we consider relevant for our task.\n",
    "\n",
    "Below, we use scikit-learn to convert the injury narratives from our training data into a matrix (table of numbers), with each row of that matrix corresponding to the vector produced by our bag-of-words representation of an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "# Use the narratives in our training data to create the vocabulary that will\n",
    "# be represented by our feature vectors. This is remembered by the vectorizer.\n",
    "vectorizer.fit(df_training['NARRATIVE'])\n",
    "# Convert the training narratives into their matrix representation.\n",
    "x_training = vectorizer.transform(df_training['NARRATIVE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, an examination of the feature matrix shows the number of training examples and features (i.e. words in our vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples: 18681\n",
      "# of features representing each example: 11915\n"
     ]
    }
   ],
   "source": [
    "n_rows = x_training.shape[0]\n",
    "n_features = x_training.shape[1]\n",
    "print('# of training examples:', n_rows)\n",
    "print('# of features representing each example:', n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the matrix directly but it's not very interesting. Most of the elements of our feature matrix are 0 for any given example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector representing our first training narrative looks like this: [[0 0 0 ..., 0 0 0]]\n",
      "Only 29 of the 11915 elements in this vector are nonzero\n"
     ]
    }
   ],
   "source": [
    "dense_vector = x_training[0].todense()\n",
    "print('The vector representing our first training narrative looks like this:', dense_vector)\n",
    "print('Only %s of the %s elements in this vector are nonzero' % (np.count_nonzero(dense_vector), n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting and Fitting the Model\n",
    "Another important consideration in building a machine learning autocoder is selecting and fitting our model. The model defines the ways in which our features can relate to the codes that will be assigned, our fitting procedure decides how the details of the model will ultimately be learned.\n",
    "\n",
    "Below, we use a regularized Logistic Regression model, which has been shown\n",
    "to work well on many text classification tasks. The regularization hyperparameter C controls how closely we allow the model to fit the training data. If C is too high the model will tend to \"overfit\". If it is too low, the model may fail to learn important relationships between the features and the codes. \n",
    "\n",
    "There is only one way to find the optimal value for C: experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# y_training contains the codes associated with our training narratives\n",
    "y_training = df_training['INJ_BODY_PART_CD']\n",
    "# we create an instance of the LogisticRegression model\n",
    "clf = LogisticRegression(C=1.0)\n",
    "# we fit the model to our training data (ie. we calculate the model parameters)\n",
    "clf.fit(x_training, y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like our other outputs, we can inspect the Logistic Regression model. The most important attribute of the model is the coefficients, which show how strongly our various features are related to our various codes. Like our feature values, these coefficients are also stored in a matrix. \n",
    "\n",
    "Below, we see that the coefficients matrix consists of 46 rows, one for each of the possible codes, and 12,441 columns, one for each of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46L, 11915L)\n",
      "[ -1.50909819e-03  -1.26303852e-04  -2.11726350e-06 ...,  -1.06079037e-02\n",
      "  -1.52733803e-04  -3.74806034e-03]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_.shape)\n",
    "print(clf.coef_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Already we have seen that there are many different ways to build our autocoder, the regularization parameter (C) alone has infinite values. In fact, there are many other things we can vary, some of which we will discuss later. To choose amongst these options we must try them and measure their quality in some way. This is the purpose of our validation data.\n",
    "\n",
    "Below, we use our vectorizer to convert the narratives from our valadition data into into a feature matrix using the same encoding scheme used for our training data. We then use our trained LogisticRegression classifier to predict codes for these narratives, and compare these to the \"true\" codes already assigned to the narratives by MSHA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.758082373782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the validation narratives to a feature matrix\n",
    "x_validation = vectorizer.transform(df_validation['NARRATIVE'])\n",
    "# Generate predicted codes for our validation narratives\n",
    "y_validation_pred = clf.predict(x_validation)\n",
    "# Calculate how accurately these match the true codes\n",
    "y_validation = df_validation['INJ_BODY_PART_CD']\n",
    "accuracy = accuracy_score(y_validation, y_validation_pred)\n",
    "print('accuracy = %s' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation data says we have 75% accuracy, not bad considering we've done almost no work. It's perfectly possible that human accuracy is near 75% on this task, although that's a study for another time. \n",
    "\n",
    "What if we had been foolish and evaluated our model only on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.953749799261\n"
     ]
    }
   ],
   "source": [
    "y_training_pred = clf.predict(x_training)\n",
    "accuracy = accuracy_score(y_training, y_training_pred)\n",
    "print('accuracy = %s' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! Evaluating on our training data would have told us we were coding at 95% accuracy when really we only get 75% accuracy on the validation data. A large gap between accuracy on the training data and validation data is an indicator that overfitting is occurring, and 20% is a very large gap. This suggests we might be able to improve our model by reducing overfitting.\n",
    "\n",
    "There are a number of ways to do this: we might reduce the number of features in our model, we might gather more training data, or we might use a different model that is more resistant to overfitting. In this case we will reduce the regularization hyperparameter C, which tends to also reduce overfitting. Below, we train a new model with C set at 0.1 and re-evaluate it's performance on our training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data is: 0.843156147958\n",
      "accuracy on validation data is: 0.763175376439\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1)\n",
    "clf.fit(x_training, y_training)\n",
    "\n",
    "y_training_pred = clf.predict(x_training)\n",
    "training_accuracy = accuracy_score(y_training, y_training_pred)\n",
    "print('accuracy on training data is: %s' % training_accuracy)\n",
    "\n",
    "y_validation_pred = clf.predict(x_validation)\n",
    "validation_accuracy = accuracy_score(y_validation, y_validation_pred)\n",
    "print('accuracy on validation data is: %s' % validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the hyperparameter C has reduced our overfitting quite a bit. The accuracy on the training data has fallen substantially (but who cares, we don't need to predict data we already know the codes for). More importantly, our accuracy on the validation data seems to have improved slightly. This is likely to be a better model than what we were using previously.\n",
    "\n",
    "There are many other ways we might adjust our model. For example, we might change the way we convert our narratives into feature vectors. This is controlled by the vectorizer we use, in this case CountVectorizer. The [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVr.html) online shows there are many ways this might be modified, we illustrate a few below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minimum document frequency\n",
    "By default, CountVectorizer includes any feature (i.e. word) that occurs in our training data as a feature in our feature vector. We can use the min_df argument to specify that features should only be included in the feature vector if they occur in at least X different documents. Below, we specify that a feature should only be included if it occurs in at least 2 different documents. Previously this resulted in a feature vector that included 12,336 features. Now, we only get 7,163."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21035, 7163)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer1 = CountVectorizer(min_df=2)\n",
    "x_training = vectorizer1.fit_transform(df_training['NARRATIVE'])\n",
    "x_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "Another CountVectorizer default is to only include individual words as features. Sometimes it's also useful to include short sequences of words, 2 word sequences (called bigrams) and ocassionaly, 3 word sequences called trigrams. Below, we tell the vectorizer to create features for all individual words and word pairs that occur in at least 2 examples in our training data. This has the effect of dramatically increasing the number of features in our feature vector to 56,853! Be careful with this, you can very easily end up with a feature matrix larger than your computer can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21035, 56853)\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(min_df=2, ngram_range=(1,2))\n",
    "x_training = vectorizer2.fit_transform(df_training['NARRATIVE'])\n",
    "print(x_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a typical machine learning project you will likely spend a lot of time tweaking your model and retraining it, as we have done above. Eventually you will settle on the best version (as measured against your validation data). You will then measure it's performance on the test data to get a final measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "\n",
    "Suppose we have settled on our final autocoder. This may have taken quite a bit of time to train and develop. Some models can take days, weeks, or even months to train. To save it for later, we need to save both the vectorizer used to encode our data and the model used to classify it. Below, we do this using the Python module `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# save the classifier object as LRclf.pkl\n",
    "joblib.dump(clf, filename='LRclf.pkl')\n",
    "# save the vectorizer object as vectorizer.pkl\n",
    "joblib.dump(vectorizer, filename='vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we can load these back as shown below. Note: we must first import any objects or functions used by our pickled files, otherwise they will fail to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# import the objects used by our saved vectorizer and classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# load the classifier\n",
    "clf = joblib.load(filename='LRclf.pkl')\n",
    "# load the vectorizer\n",
    "vectorizer = joblib.load(filename='vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Autocoder\n",
    "\n",
    "There are a number of ways we might use our newly created autocoder. The simplest option is to just automatically assign the codes. Suppose for example, that our test dataset is really just a set of uncoded narratives. We might code it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCIDENT_DT</th>\n",
       "      <th>FIPS_STATE_CD</th>\n",
       "      <th>INJ_BODY_PART</th>\n",
       "      <th>INJ_BODY_PART_CD</th>\n",
       "      <th>MINE_ID</th>\n",
       "      <th>NARRATIVE</th>\n",
       "      <th>Autocode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35783</th>\n",
       "      <td> 2013-03-11 00:00:00</td>\n",
       "      <td>  1</td>\n",
       "      <td>                         KNEE/PATELLA</td>\n",
       "      <td> 512</td>\n",
       "      <td>  101401</td>\n",
       "      <td> Employee was walking by a supply car when he s...</td>\n",
       "      <td> 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40528</th>\n",
       "      <td> 2010-09-11 00:00:00</td>\n",
       "      <td> 32</td>\n",
       "      <td> MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
       "      <td> 700</td>\n",
       "      <td> 2601089</td>\n",
       "      <td> Employee in bus when bus hit a berm.  Diagnose...</td>\n",
       "      <td> 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29973</th>\n",
       "      <td> 2014-07-21 00:00:00</td>\n",
       "      <td> 54</td>\n",
       "      <td> BACK (MUSCLES/SPINE/S-CORD/TAILBONE)</td>\n",
       "      <td> 420</td>\n",
       "      <td> 4609086</td>\n",
       "      <td> Employee stated that he completed performing m...</td>\n",
       "      <td> 420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38708</th>\n",
       "      <td> 2011-02-01 00:00:00</td>\n",
       "      <td> 21</td>\n",
       "      <td>          HAND (NOT WRIST OR FINGERS)</td>\n",
       "      <td> 330</td>\n",
       "      <td> 1519447</td>\n",
       "      <td> Cleaning back board off at #2 tailpiece with a...</td>\n",
       "      <td> 330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324 </th>\n",
       "      <td> 2013-01-09 00:00:00</td>\n",
       "      <td> 48</td>\n",
       "      <td> BACK (MUSCLES/SPINE/S-CORD/TAILBONE)</td>\n",
       "      <td> 420</td>\n",
       "      <td> 4100991</td>\n",
       "      <td> Miner had finished cleaning up a wet concrete ...</td>\n",
       "      <td> 420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ACCIDENT_DT  FIPS_STATE_CD  \\\n",
       "35783  2013-03-11 00:00:00              1   \n",
       "40528  2010-09-11 00:00:00             32   \n",
       "29973  2014-07-21 00:00:00             54   \n",
       "38708  2011-02-01 00:00:00             21   \n",
       "9324   2013-01-09 00:00:00             48   \n",
       "\n",
       "                              INJ_BODY_PART  INJ_BODY_PART_CD  MINE_ID  \\\n",
       "35783                          KNEE/PATELLA               512   101401   \n",
       "40528  MULTIPLE PARTS (MORE THAN ONE MAJOR)               700  2601089   \n",
       "29973  BACK (MUSCLES/SPINE/S-CORD/TAILBONE)               420  4609086   \n",
       "38708           HAND (NOT WRIST OR FINGERS)               330  1519447   \n",
       "9324   BACK (MUSCLES/SPINE/S-CORD/TAILBONE)               420  4100991   \n",
       "\n",
       "                                               NARRATIVE  Autocode  \n",
       "35783  Employee was walking by a supply car when he s...       512  \n",
       "40528  Employee in bus when bus hit a berm.  Diagnose...       100  \n",
       "29973  Employee stated that he completed performing m...       420  \n",
       "38708  Cleaning back board off at #2 tailpiece with a...       330  \n",
       "9324   Miner had finished cleaning up a wet concrete ...       420  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = vectorizer.transform(df_test['NARRATIVE'])\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "df_test['Autocode'] = y_test_pred\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to be more selective. Our LogisticRegression model has the nice property that it can tell us not only which codes it thinks are best, but it can also give us an estimate of how likely those codes are to be correct. We might use this information to only assign codes that exceed a certain likelihood of being correct. Alternatively, we could use this information to \"suggest\" likely codes to a human coder.\n",
    "\n",
    "To get classification probabilities we use the predict_proba() method of our Logistic Regression classifier. For each of the examples in the matrix fed to predict_proba() we get a sequence showing how confident the model is that each of the 46 possible codes should be assigned to it. \n",
    "\n",
    "Below, we run predict_proba() on our 10,000 example test dataset. The result is a 10,000 row matrix (one for each example), with 46 columns (one for each code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the pred_prob matrix is: (10000L, 46L)\n",
      "The probabilities for the first example are:\n",
      "[  3.41020511e-03   3.01536102e-03   1.34404050e-03   2.73822059e-03\n",
      "   3.34934214e-03   3.72835828e-03   2.71803865e-03   1.34908277e-03\n",
      "   2.82138263e-03   8.00753984e-04   1.65381330e-03   7.07661375e-04\n",
      "   1.21763663e-03   1.38525872e-03   5.37209415e-03   1.09459687e-03\n",
      "   4.24352843e-03   3.76646598e-03   3.92432816e-03   4.05624446e-03\n",
      "   1.43570253e-03   5.17993260e-03   5.10698189e-03   5.36387885e-03\n",
      "   2.79492518e-03   1.31328913e-03   2.39314012e-03   2.79896495e-03\n",
      "   6.85283046e-03   4.13963828e-03   6.14814400e-03   2.17366664e-03\n",
      "   1.50971133e-03   6.02092928e-03   9.47741545e-03   7.96389434e-01\n",
      "   1.27997256e-02   4.67647647e-03   3.75432874e-02   5.10830117e-03\n",
      "   2.82194370e-03   3.30385409e-03   7.88862252e-03   1.19017681e-02\n",
      "   1.02380146e-03   1.13722073e-03]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = clf.predict_proba(x_test)\n",
    "print('The shape of the pred_prob matrix is: %s' % str(y_pred_prob.shape))\n",
    "print('The probabilities for the first example are:\\n%s' % y_pred_prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position in the vector indicates the code the probability corresponds to. To recover just the probability and position of the most likely code, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INJ_BODY_PART</th>\n",
       "      <th>INJ_BODY_PART_CD</th>\n",
       "      <th>Autocode</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35783</th>\n",
       "      <td>                         KNEE/PATELLA</td>\n",
       "      <td> 512</td>\n",
       "      <td> 512</td>\n",
       "      <td> 0.796389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40528</th>\n",
       "      <td> MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
       "      <td> 700</td>\n",
       "      <td>    </td>\n",
       "      <td> 0.489486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29973</th>\n",
       "      <td> BACK (MUSCLES/SPINE/S-CORD/TAILBONE)</td>\n",
       "      <td> 420</td>\n",
       "      <td>    </td>\n",
       "      <td> 0.723080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38708</th>\n",
       "      <td>          HAND (NOT WRIST OR FINGERS)</td>\n",
       "      <td> 330</td>\n",
       "      <td> 330</td>\n",
       "      <td> 0.847322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324 </th>\n",
       "      <td> BACK (MUSCLES/SPINE/S-CORD/TAILBONE)</td>\n",
       "      <td> 420</td>\n",
       "      <td>    </td>\n",
       "      <td> 0.132973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              INJ_BODY_PART  INJ_BODY_PART_CD Autocode  \\\n",
       "35783                          KNEE/PATELLA               512      512   \n",
       "40528  MULTIPLE PARTS (MORE THAN ONE MAJOR)               700            \n",
       "29973  BACK (MUSCLES/SPINE/S-CORD/TAILBONE)               420            \n",
       "38708           HAND (NOT WRIST OR FINGERS)               330      330   \n",
       "9324   BACK (MUSCLES/SPINE/S-CORD/TAILBONE)               420            \n",
       "\n",
       "       Probability  \n",
       "35783     0.796389  \n",
       "40528     0.489486  \n",
       "29973     0.723080  \n",
       "38708     0.847322  \n",
       "9324      0.132973  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a sequence indicating the position with the highest probability for each row\n",
    "top_positions = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "codes = []\n",
    "probabilities = []\n",
    "# loop through each example and retrieve the best code and it's probability\n",
    "# if the probability exceeds .75, add it as an autocode, otherwise leave it blank\n",
    "for n, pos in enumerate(top_positions):\n",
    "    # grab the code that corresponds to the position with the highest probability\n",
    "    code = clf.classes_[pos]\n",
    "    # grab the probability at that position\n",
    "    prob = y_pred_prob[n][pos]\n",
    "    # add the probability to our list of probabilities\n",
    "    probabilities.append(prob)\n",
    "    # if prob exceeds our cutoff add the code to our list of codes\n",
    "    if prob > .75:\n",
    "        codes.append(code)\n",
    "    # if prob does not exceed the cutoff, add a blank to our list of codes\n",
    "    else:\n",
    "        codes.append('')\n",
    "\n",
    "# Add the autocodes to our DataFrame\n",
    "df_test['Autocode'] = codes\n",
    "# Add the probabilities to our DataFrame\n",
    "df_test['Probability'] = probabilities\n",
    "# Show selected columns of our updated DataFrame\n",
    "df_test[['INJ_BODY_PART', 'INJ_BODY_PART_CD', 'Autocode', 'Probability']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras\n",
    "\n",
    "Scikit-learn provides a wide variety of powerful tools that we didn't have time to cover in this class, including many different learning algorithms and utilities for feature and model selection. To learn more, see the official and very extensive [online documentation](http://scikit-learn.org/stable/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
