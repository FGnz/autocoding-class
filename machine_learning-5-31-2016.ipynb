{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the Training and Evaluation Data\n",
    "\n",
    "We will use the msha.csv data set to build and evaluate our autocoder. We begin by dividing it into 3 pieces, each of which will be used for a separate purpose. These pieces are training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read the csv file into a data frame, a special Python object for representing tabular data\n",
    "df = pd.read_csv(r'msha.csv', parse_dates=['ACCIDENT_DT'])\n",
    "\n",
    "# add a column to our data frame with the year the incident occurred\n",
    "df['YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
    "\n",
    "# split the data frame into separate test, validation, and training dataframes based on the year of the incident\n",
    "df_training = df[df['YEAR'] <= 2011]\n",
    "df_validation = df[df['YEAR'] == 2012]\n",
    "df_test = df[df['YEAR'] == 2013]\n",
    "\n",
    "print('''Data set sizes:\n",
    "    test: %s\n",
    "    validation: %s\n",
    "    training: %s''' % (len(df_test), len(df_validation), len(df_training)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting inputs to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the narratives in our training data to create the vocabulary that will\n",
    "# be represented by our feature vectors. This is remembered by the vectorizer.\n",
    "vectorizer.fit(df_training['NARRATIVE'])\n",
    "\n",
    "print('Our vectorizer has defined an input vector with %s elements' % len(vectorizer.vocabulary_))\n",
    "pprint(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined this mapping of inputs to vector positions we can convert our training data to vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the training narratives into their matrix representation.\n",
    "x_training = vectorizer.transform(df_training['NARRATIVE'])\n",
    "\n",
    "print(x_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the matrix directly but it's not very interesting. Most of the elements of our feature matrix are 0 for any given example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = x_training.shape[1]\n",
    "dense_vector = x_training[0].todense()\n",
    "print('The vector representing our first training narrative looks like this:', dense_vector)\n",
    "print('Only %s of the %s elements in this vector are nonzero' % (np.count_nonzero(dense_vector), n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector = vectorizer.transform(['zipties zone'])\n",
    "print(vector.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting and Fitting the Model\n",
    "\n",
    "We have converted our training data to a feature matrix, we can now select our model and fit it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# y_training contains the codes associated with our training narratives\n",
    "y_training = df_training['INJ_BODY_PART_CD']\n",
    "\n",
    "# we create an instance of the LogisticRegression model and set regularization to 1.0\n",
    "clf = LogisticRegression(C=10)\n",
    "\n",
    "# we fit the model to our training data (ie. we calculate the weights)\n",
    "clf.fit(x_training, y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like our other outputs, we can inspect the Logistic Regression model. The most important attribute of the model is the coefficients, which show how strongly our various features are related to our various codes. Like our feature values, these coefficients are also stored in a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clf.coef_.shape)\n",
    "print(clf.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can look at which codes each of these rows correspond to, and the features that have the highest weights (make that class much more likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the codes recognized by the classifier\n",
    "print(clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine these weights more closely and see which inputs they correspond to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the index of the code whose weights we want to examine \n",
    "# index 0 corresponds to code 100 (head not elsewhere classified)\n",
    "# index 3 corresponds to code 121 (ear, external)\n",
    "code_index = 0\n",
    "code = clf.classes_[code_index]\n",
    "\n",
    "# Retrieve the weights for the specified code_index\n",
    "code_weights = clf.coef_[code_index]\n",
    "\n",
    "# find the feature_index with largest weight for this code\n",
    "feature_index = code_weights.argmax()\n",
    "\n",
    "# create a dictionary that maps from index, to word\n",
    "feature_mapper = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# map that index to the word it represents\n",
    "word = feature_mapper[feature_index]\n",
    "print('The word with the heighest weight for code %s is \"%s\"' % (code, word))\n",
    "print('It has a weight of:', clf.coef_[code_index, feature_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Convert the validation narratives to a feature matrix\n",
    "x_validation = vectorizer.transform(df_validation['NARRATIVE'])\n",
    "\n",
    "# Generate predicted codes for our validation narratives\n",
    "y_validation_pred = clf.predict(x_validation)\n",
    "\n",
    "# Calculate how accurately these match the true codes\n",
    "y_validation = df_validation['INJ_BODY_PART_CD']\n",
    "accuracy = accuracy_score(y_validation, y_validation_pred)\n",
    "macro_f1 = f1_score(y_validation, y_validation_pred, average='macro')\n",
    "print('accuracy = %s' % (accuracy))\n",
    "print('macro f1 score = %s' % (macro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had been foolish and evaluated our performance on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_training_pred = clf.predict(x_training)\n",
    "accuracy = accuracy_score(y_training, y_training_pred)\n",
    "macro_f1 = f1_score(y_training, y_training_pred, average='macro')\n",
    "print('accuracy = %s' % (accuracy))\n",
    "print('macro f1 score = %s' % (macro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Different Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(x_training, y_training)\n",
    "\n",
    "y_training_pred = clf.predict(x_training)\n",
    "training_accuracy = accuracy_score(y_training, y_training_pred)\n",
    "print('accuracy on training data is: %s' % training_accuracy)\n",
    "\n",
    "y_validation_pred = clf.predict(x_validation)\n",
    "validation_accuracy = accuracy_score(y_validation, y_validation_pred)\n",
    "print('accuracy on validation data is: %s' % validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minimum document frequency\n",
    "By default, CountVectorizer includes any feature (i.e. word) that occurs in our training data as a feature in our feature vector. We can use the min_df argument to specify that features should only be included in the feature vector if they occur in at least X different documents. Below, we specify that a feature should only be included if it occurs in at least 2 different documents. Previously this resulted in a feature vector that included 12,336 features. Now, we only get 7,163."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(min_df=2)\n",
    "x_training = vectorizer1.fit_transform(df_training['NARRATIVE'])\n",
    "x_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "Using just the occurrence of individual words (unigrams) loses word order, which might be useful. We can counteract that by using multiple word sequences, such as bigrams (2 word sequences), trigrams (3 word sequences), and so on. In practice it is rarely useful to go beyond bigrams for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(min_df=5, ngram_range=(1,2))\n",
    "vectorizer2.fit(df_training['NARRATIVE'])\n",
    "print(len(vectorizer2.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a typical machine learning project you will likely spend a lot of time tweaking your model and retraining it, as we have done above. Eventually you will settle on the best version (as measured against your validation data). You will then measure it's performance on the test data to get a final measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_training = vectorizer2.transform(df_training['NARRATIVE'])\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(x_training, y_training)\n",
    "y_training_pred = clf.predict(x_training)\n",
    "training_accuracy = accuracy_score(y_training, y_training_pred)\n",
    "print('accuracy on training data is: %s' % training_accuracy)\n",
    "\n",
    "x_validation = vectorizer2.transform(df_validation['NARRATIVE'])\n",
    "y_validation_pred = clf.predict(x_validation)\n",
    "validation_accuracy = accuracy_score(y_validation, y_validation_pred)\n",
    "print('accuracy on validation data is: %s' % validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "\n",
    "Suppose we have settled on our final autocoder. This may have taken quite a bit of time to train and develop. Some models can take days, weeks, or even months to train. To save it for later, we need to save both the vectorizer used to encode our data and the model used to classify it. Below, we do this using the Python module `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# save the classifier object as LRclf.pkl\n",
    "joblib.dump(clf, filename='LRclf.pkl')\n",
    "# save the vectorizer object as vectorizer.pkl\n",
    "joblib.dump(vectorizer2, filename='vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we can load these back as shown below. Note: we must first import any objects or functions used by our pickled files, otherwise they will fail to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# import the objects used by our saved vectorizer and classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# load the classifier\n",
    "clf = joblib.load(filename='LRclf.pkl')\n",
    "# load the vectorizer\n",
    "vectorizer = joblib.load(filename='vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Autocoder\n",
    "\n",
    "There are a number of ways we might use our newly created autocoder. The simplest option is to just automatically assign the codes. Suppose for example, that our test dataset is really just a set of uncoded narratives. We might code it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = vectorizer.transform(df_test['NARRATIVE'])\n",
    "y_test_pred = clf.predict(x_test)\n",
    "print(x_test.shape)\n",
    "print(y_test_pred.shape)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assigning codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test.loc[:, ('Autocode')] = y_test_pred\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accessing the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_prob = clf.predict_proba(x_test)\n",
    "print('The shape of the pred_prob matrix is: %s' % str(y_pred_prob.shape))\n",
    "print('The probabilities for the first example are:\\n%s' % y_pred_prob[0])\n",
    "max_index = y_pred_prob[0].argmax()\n",
    "code = clf.classes_[max_index]\n",
    "print('The highest probability is at index %s which corresponds to code %s' % (max_index, code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding the predicted probability for each code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a sequence indicating the position with the highest probability for each row\n",
    "top_positions = y_pred_prob.argmax(axis=1)\n",
    "top_probabilities = y_pred_prob[np.arange(len(top_positions)), top_positions]\n",
    "print(top_probabilities.shape)\n",
    "df_test.loc[:, ('Probability')] = top_probabilities\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.to_excel('msha_autocoded.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras\n",
    "\n",
    "Scikit-learn provides a wide variety of powerful tools that we didn't have time to cover in this class, including many different learning algorithms and utilities for feature and model selection. To learn more, see the official and very extensive [online documentation](http://scikit-learn.org/stable/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
