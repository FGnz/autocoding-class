{
 "metadata": {
  "name": "",
  "signature": "sha256:3f199e519f76b123130120a3e7fd92d6fa31aeefd7919b5f275d280a75850ad1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Experimental Setup\n",
      "Once you have gathered the data for your autocoder, the first thing you should do is randomly divide it into 3 datasets: training, validation, and test. The training data will be used to train a variety of different autocoders, the validation data will be used to choose the best of these, and the test data will only be used (ideally, once) at the very end to measure how well your best autocoder works.\n",
      "\n",
      "It's important to do this because the techniques we will be using have a tremendous ability to learn, so much so that in some cases they can simply memorize the training data while learning little about how to code future cases. For example, it might see that the name \"Wiatt Springfield\" is only associated with a \"foot\" injury in the training data and assume this is a perfect indicator of whether someone injured their foot. This phenomenon is called \"overfitting\" and it is the bane of many a researcher. Our setup helps us detect and prevent this.  \n",
      "\n",
      "The exact ratio of cases allocated to the training, validation, and test datasets is usually not too important, a 50%/25%/25% split is not unusual, nor is a 90%/5%/5% split. Ultimately, the tradeoff is that more training data allows you to build better initial models, more validation data allows you to more precisely determine the best of your competing models, and more test data allows you to more precisely measure the quality of your final model. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below, we read a csv file into a DataFrame, randomly permute the indexes of that DataFrame, and then set the first 10,000 rows aside for test data, the next 10,000 for validation, and the remaining 20,000+ for training."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# read the csv file into a DataFrame\n",
      "df = pd.read_csv(r'C:\\Users\\measure_a\\Desktop\\autocoding-class\\msha.csv')\n",
      "\n",
      "# randomly permute the indexes of the DataFrame\n",
      "df.reindex(np.random.permutation(df.index))\n",
      "\n",
      "# split the DataFrame into separate test, validation, and training DataFrames\n",
      "df_test = df[0:10000]\n",
      "df_validation = df[10000:20000]\n",
      "df_training = df[20000:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 211
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Feature Selection\n",
      "A critical step in building a machine learning autocoder is determining the relevant inputs for the task and then figuring out how to encode them in a way that will be \"learnable\" by one of our machine learning techniques. In practice this is where many experts spend much of their time as they try a huge number of variations. \n",
      "\n",
      "As it happens, there is a very simple approach for text classification that will often get you state of the art performance, or at least very close to it. It's called the bag-of-words-representation.\n",
      "\n",
      "The basic idea is to represent each example with a vector (a list of numbers), where each position in that vector corresponds to a word that occurs in your examples, and the value at that position indicates something about that word's occurence. Often, we use a 1 to indicate that the word is present, and a 0 to indicate it is not.\n",
      "\n",
      "Below, we use scikit-learn to convert a long list of injury narratives into a matrix, with each row of that matrix corresponding to the vector produced by our bag-of-words representation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Create an instance of the CountVectorizer object\n",
      "vectorizer = CountVectorizer()\n",
      "# Use the narratives in our training data to create the vocabulary that will\n",
      "# be represented by our feature vectors. This is remembered by the vectorizer.\n",
      "vectorizer.fit(df_training['NARRATIVE'])\n",
      "# Convert the training narratives into their matrix representation.\n",
      "x_training = vectorizer.transform(df_training['NARRATIVE'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 212
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Below, an examination of the feature matrix shows it consists of 21,035 rows (feature vectors representing examples), and 12,441 columns (words in our vocabulary)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(x_training.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(21035, 12441)\n"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also examine the matrix directly, but is rather difficult to decipher. Virtually all of the entries are 0 since most words do not occur in most of our examples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' \n",
      "The first row of our matrix, i.e. the vector representing our first \n",
      "training example. All the visible entries are 0, which is not unexpected.\n",
      "Of the 12,441 elements in this vector, only 18 have a non-zero value.\n",
      "'''\n",
      "x_training[0].todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 217,
       "text": [
        "matrix([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Selecting and fitting the Model\n",
      "Another important consideration in building a machine learning autocoder is selecting a model and deciding how we will fit it. The model defines the ways in which our features can relate to the codes that will be assigned, our fitting procedure decides how the details of the model will ultimately be learned.\n",
      "\n",
      "Below, we use a regularized Logistic Regression model, which has been shown\n",
      "to work well on many text classification tasks. The regularization parameter C controls how closely we allow the model to fit the training data. If C is too high the model may \"overfit\". If it is too low, the model may fail to learn important relationships between the features and the codes. There is only one way to find out: experimentation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "# y_training contains the codes associated with our training narratives\n",
      "y_training = df_training['INJ_BODY_PART_CD']\n",
      "# we create an instance of the LogisticRegression model\n",
      "clf = LogisticRegression()\n",
      "# we fit the model to our training data (ie. we calculate the model parameters)\n",
      "clf.fit(x_training, y_training)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 218,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Like our other outputs, we can inspect the Logistic Regression model. The most important attribute of the model is the coefficients, which show how strongly our various features are related to our various codes. Like our features, these coefficients are also stored in a matrix. We see below that it consists of 45 rows, one for each of the possible codes, and 12,441 columns, one for each of our features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(clf.coef_.shape)\n",
      "print(clf.coef_[2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(45L, 12441L)\n",
        "[ -1.49989797e-03  -8.32565807e-06  -2.57146849e-07 ...,  -4.23024936e-04\n",
        "   3.20631484e-08   3.24435833e-09]\n"
       ]
      }
     ],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Evaluation\n",
      "\n",
      "TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "df.reindex(np.random.permutation(df.index))\n",
      "df_test = df[0:10000]\n",
      "df_validation = df[10000:20000]\n",
      "df_training = df[20000:]\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer(min_df=10)\n",
      "x_training = vectorizer.fit_transform(df_training['NARRATIVE'])\n",
      "print(x_training.shape)\n",
      "\n",
      "y_training = df_training['INJ_BODY_PART_CD']\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression()\n",
      "clf.fit(x_training, y_training)\n",
      "\n",
      "pred_y = clf.predict(x_training)\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "print(accuracy_score(y_training, pred_y))\n",
      "\n",
      "x_validation = vectorizer.transform(df_validation['NARRATIVE'])\n",
      "y_validation = df_validation['INJ_BODY_PART_CD']\n",
      "\n",
      "valid_pred_y = clf.predict(x_validation)\n",
      "print(accuracy_score(y_validation, valid_pred_y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(21035, 3073)\n",
        "0.938388400285"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.7474"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "train_validation, test = train_test_split(df, test_size=10000)\n",
      "train, validation = train_test_split(train_validation, test_size=10000)\n",
      "print(len(test))\n",
      "print(len(validation))\n",
      "print(len(train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000\n",
        "10000\n",
        "21035\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "train_narratives = train[:, 5]\n",
      "\n",
      "vectorizer = CountVectorizer(min_df=2)\n",
      "train_x = vectorizer.fit_transform(train_narratives)\n",
      "train_x.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "(21035, 7151)"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "train_codes = train[:, 3]\n",
      "\n",
      "label_encoder = LabelEncoder()\n",
      "train_y = label_encoder.fit_transform(train_codes.tolist())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_y.shape\n",
      "print train_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0 29 43 ..., 22 16 27]\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "clf = LogisticRegression(C=.01)\n",
      "clf.fit(train_x, train_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(clf.coef_.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(46L, 7151L)\n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "validation_x = vectorizer.transform(validation[:, 5])\n",
      "validation_x.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "(10000, 7151)"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred_y = clf.predict(validation_x)\n",
      "pred_y[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 112,
       "text": [
        "23"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "validation_y = label_encoder.transform(validation[:, 3])\n",
      "validation_y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 113,
       "text": [
        "(10000L,)"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(validation_y)\n",
      "names = [code_map[i] for i in label_encoder.classes_]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[23 33 29 ..., 23 27 16]\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "z=classification_report(validation_y, pred_y, target_names=names)\n",
      "print(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                         precision    recall  f1-score   support\n",
        "\n",
        "                               HEAD,NEC       0.52      0.55      0.53       229\n",
        "                                  BRAIN       0.00      0.00      0.00        66\n",
        "             EAR(S) INTERNAL & EXTERNAL       1.00      0.12      0.21        34\n",
        "                        EAR(S) EXTERNAL       0.93      0.86      0.90       131\n",
        "              EAR(S) INTERNAL & HEARING       0.83      0.93      0.88       472\n",
        "               EYE(S) OPTIC NERVE/VISON       0.81      0.21      0.33        63\n",
        "                               FACE,NEC       0.00      0.00      0.00        35\n",
        "                       JAW INCLUDE CHIN       0.66      0.78      0.71       169\n",
        "    MOUTH/LIP/TEETH/TONGUE/THROAT/TASTE       1.00      0.17      0.29        47\n",
        "        NOSE/NASAL PASSAGES/SINUS/SMELL       0.00      0.00      0.00        49\n",
        "                   FACE, MULTIPLE PARTS       0.00      0.00      0.00        15\n",
        "                                  SCALP       0.00      0.00      0.00         2\n",
        "                                  SKULL       0.00      0.00      0.00        15\n",
        "                   HEAD, MULTIPLE PARTS       0.78      0.40      0.53       168\n",
        "                                   NECK       0.00      0.00      0.00         1\n",
        "                 UPPER EXTREMITIES, NEC       0.50      0.36      0.42       151\n",
        "                                ARM,NEC       1.00      0.09      0.16        78\n",
        "                      UPPER ARM/HUMERUS       0.87      0.67      0.76       135\n",
        "                                  ELBOW       0.83      0.66      0.73       229\n",
        "                   FOREARM/ULNAR/RADIUS       0.00      0.00      0.00         8\n",
        "                    ARM, MULTIPLE PARTS       0.90      0.77      0.83       300\n",
        "                                  WRIST       0.63      0.74      0.68       572\n",
        "            HAND (NOT WRIST OR FINGERS)       0.79      0.91      0.85      1702\n",
        "                        FINGER(S)/THUMB       0.00      0.00      0.00        55\n",
        "            UPPER EXTREMITIES, MULTIPLE       0.00      0.00      0.00         3\n",
        "                              TRUNK,NEC       0.00      0.00      0.00        58\n",
        "                ABDOMEN/INTERNAL ORGANS       0.61      0.89      0.72      1178\n",
        "   BACK (MUSCLES/SPINE/S-CORD/TAILBONE)       0.78      0.50      0.61       303\n",
        "   CHEST (RIBS/BREAST BONE/CHEST ORGNS)       0.78      0.57      0.66       216\n",
        "  HIPS (PELVIS/ORGANS/KIDNEYS/BUTTOCKS)       0.80      0.87      0.83       650\n",
        "SHOULDERS (COLLARBONE/CLAVICLE/SCAPULA)       0.00      0.00      0.00        87\n",
        "                  TRUNK, MULTIPLE PARTS       0.55      0.29      0.38       145\n",
        "                  LOWER EXTREMITIES,NEC       0.73      0.08      0.15        95\n",
        "                               LEG, NEC       0.85      0.91      0.88       741\n",
        "                            THIGH/FEMUR       0.68      0.35      0.46       208\n",
        "                           KNEE/PATELLA       0.00      0.00      0.00        11\n",
        "                 LOWER LEG/TIBIA/FIBULA       0.83      0.86      0.84       429\n",
        "                    LEG, MULTIPLE PARTS       0.66      0.80      0.73       268\n",
        "                                  ANKLE       1.00      0.08      0.15        38\n",
        "  FOOT(NOT ANKLE/TOE)/TARSUS/METATARSUS       0.00      0.00      0.00        47\n",
        "                       TOE(S)/PHALANGES       0.65      0.49      0.56       202\n",
        "      LOWER EXTREMITIES, MULTIPLE PARTS       0.28      0.37      0.32       583\n",
        "                           BODY SYSTEMS       0.00      0.00      0.00        12\n",
        "\n",
        "                            avg / total       0.69      0.70      0.67     10000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "accuracy = accuracy_score(validation_y, pred_y)\n",
      "print('accuracy = %s' % accuracy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy = 0.7012\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idx = 10\n",
      "predicted = pred_y[idx]\n",
      "actual = validation[idx]\n",
      "code = label_encoder.inverse_transform(predicted)\n",
      "print('predicted code: %s' % code)\n",
      "print('predicted label: %s' % code_map[code])\n",
      "print('actual code: %s' % actual[3])\n",
      "print('actual label: %s' % actual[2])\n",
      "print(actual[5])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "predicted code: 420\n",
        "predicted label: BACK (MUSCLES/SPINE/S-CORD/TAILBONE)\n",
        "actual code: 420\n",
        "actual label: BACK (MUSCLES/SPINE/S-CORD/TAILBONE)\n",
        "While cleaning a hole, the lip of the bucket caught on the rib on the way down, striking a rock, jarring the employee in the cab. He felt pain in his upper and lower back. He was examined by a physician and placed off work pending follow up.\n"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "validation[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 164,
       "text": [
        "array(['2011-05-14 00:00:00', 4L, 'LOWER LEG/TIBIA/FIBULA', 513L, 200024L,\n",
        "       \"Employee was manually moving a single lead anode on a pallet; while pulling the anode to re-position it, the hanger made contact with the employee's right shin.  The employee received stitches in his shin.\"], dtype=object)"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label_encoder.inverse_transform(23)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "340"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_code_map(df):\n",
      "    code_map = {}\n",
      "    rows = df.to_dict(outtype='records')\n",
      "    for row in rows:\n",
      "        code = row['INJ_BODY_PART_CD']\n",
      "        title = row['INJ_BODY_PART']\n",
      "        code_map[code] = title\n",
      "    return code_map"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_map = get_code_map(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_map[340]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 137,
       "text": [
        "'FINGER(S)/THUMB'"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "validation[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 128,
       "text": [
        "array(['2014-02-20 00:00:00', 42L, 'LEG, NEC', 510L, 1800729L,\n",
        "       'Employee was exiting the cab of a loader when his left foot made contact with the ground and slipped on the ice tearing the tendon in his leg.'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 130,
       "text": [
        "array([23, 39, 29, ..., 23, 27, 27])"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label_encoder.inverse_transform(23)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "340"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_map(340)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'dict' object is not callable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-132-6f85796205c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcode_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m340\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_map[340]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "'FINGER(S)/THUMB'"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label_encoder??\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idx = 2\n",
      "code = label_encoder.classes_[idx]\n",
      "label = code_map[code]\n",
      "print(code)\n",
      "print(label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "120\n",
        "EAR(S) INTERNAL & EXTERNAL\n"
       ]
      }
     ],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}